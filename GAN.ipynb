{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z4HRA-S/GAN-vs-MiniBatch-GAN/blob/master/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMRRYh5E-ySj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4956987-4098-4ca1-fcad-cff0ae761926"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (49.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=524a9901308ad2e773284c883d5468bb1d24d52d601b352acf9ede328dc359c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgooArFzAiZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72ed6c4c-84d8-4bb3-fc38-032551f7135b"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        validity = self.discriminator(img)\n",
        "\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    def build_generator(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "        model.summary()\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "        return Model(noise, img)\n",
        "        \n",
        "    def build_discriminator(self):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=self.img_shape))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "        return Model(img, validity)\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "            if epoch % sample_interval == 0:\n",
        "               self.sample_images(epoch)\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        #fig.savefig(\"images/%d.png\" % epoch)\n",
        "        plt.close()\n",
        "if __name__ == '__main__':\n",
        "    gan = GAN()\n",
        "    gan.train(epochs=1000, batch_size=132, sample_interval=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.820585, acc.: 15.15%] [G loss: 0.632238]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 [D loss: 0.417314, acc.: 58.33%] [G loss: 0.605842]\n",
            "2 [D loss: 0.375205, acc.: 67.80%] [G loss: 0.594599]\n",
            "3 [D loss: 0.367282, acc.: 69.70%] [G loss: 0.648637]\n",
            "4 [D loss: 0.356451, acc.: 74.62%] [G loss: 0.691301]\n",
            "5 [D loss: 0.330717, acc.: 85.23%] [G loss: 0.764123]\n",
            "6 [D loss: 0.317408, acc.: 89.77%] [G loss: 0.866731]\n",
            "7 [D loss: 0.289942, acc.: 92.80%] [G loss: 1.006063]\n",
            "8 [D loss: 0.250901, acc.: 98.11%] [G loss: 1.130595]\n",
            "9 [D loss: 0.224187, acc.: 99.24%] [G loss: 1.215559]\n",
            "10 [D loss: 0.192103, acc.: 99.62%] [G loss: 1.360426]\n",
            "11 [D loss: 0.169794, acc.: 99.62%] [G loss: 1.511113]\n",
            "12 [D loss: 0.145990, acc.: 100.00%] [G loss: 1.651664]\n",
            "13 [D loss: 0.134793, acc.: 100.00%] [G loss: 1.747711]\n",
            "14 [D loss: 0.119979, acc.: 100.00%] [G loss: 1.933566]\n",
            "15 [D loss: 0.105727, acc.: 100.00%] [G loss: 2.053531]\n",
            "16 [D loss: 0.093673, acc.: 100.00%] [G loss: 2.127997]\n",
            "17 [D loss: 0.087556, acc.: 100.00%] [G loss: 2.205374]\n",
            "18 [D loss: 0.081611, acc.: 100.00%] [G loss: 2.292884]\n",
            "19 [D loss: 0.077806, acc.: 100.00%] [G loss: 2.351256]\n",
            "20 [D loss: 0.067867, acc.: 100.00%] [G loss: 2.504974]\n",
            "21 [D loss: 0.059392, acc.: 100.00%] [G loss: 2.557590]\n",
            "22 [D loss: 0.059565, acc.: 100.00%] [G loss: 2.627815]\n",
            "23 [D loss: 0.052156, acc.: 100.00%] [G loss: 2.632482]\n",
            "24 [D loss: 0.047527, acc.: 100.00%] [G loss: 2.764829]\n",
            "25 [D loss: 0.044966, acc.: 100.00%] [G loss: 2.783504]\n",
            "26 [D loss: 0.047528, acc.: 100.00%] [G loss: 2.805277]\n",
            "27 [D loss: 0.042512, acc.: 100.00%] [G loss: 2.891812]\n",
            "28 [D loss: 0.041913, acc.: 100.00%] [G loss: 2.943750]\n",
            "29 [D loss: 0.040589, acc.: 100.00%] [G loss: 3.001482]\n",
            "30 [D loss: 0.037644, acc.: 100.00%] [G loss: 3.036035]\n",
            "31 [D loss: 0.033838, acc.: 100.00%] [G loss: 3.079525]\n",
            "32 [D loss: 0.036184, acc.: 100.00%] [G loss: 3.096841]\n",
            "33 [D loss: 0.032307, acc.: 100.00%] [G loss: 3.107008]\n",
            "34 [D loss: 0.033797, acc.: 100.00%] [G loss: 3.171792]\n",
            "35 [D loss: 0.030042, acc.: 100.00%] [G loss: 3.222117]\n",
            "36 [D loss: 0.032182, acc.: 100.00%] [G loss: 3.171756]\n",
            "37 [D loss: 0.027861, acc.: 100.00%] [G loss: 3.141901]\n",
            "38 [D loss: 0.028625, acc.: 100.00%] [G loss: 3.291342]\n",
            "39 [D loss: 0.033709, acc.: 100.00%] [G loss: 3.220288]\n",
            "40 [D loss: 0.026915, acc.: 100.00%] [G loss: 3.286525]\n",
            "41 [D loss: 0.028516, acc.: 100.00%] [G loss: 3.298579]\n",
            "42 [D loss: 0.025929, acc.: 100.00%] [G loss: 3.348818]\n",
            "43 [D loss: 0.024173, acc.: 100.00%] [G loss: 3.350722]\n",
            "44 [D loss: 0.028550, acc.: 100.00%] [G loss: 3.392876]\n",
            "45 [D loss: 0.028270, acc.: 100.00%] [G loss: 3.364098]\n",
            "46 [D loss: 0.027335, acc.: 100.00%] [G loss: 3.497345]\n",
            "47 [D loss: 0.026051, acc.: 100.00%] [G loss: 3.432581]\n",
            "48 [D loss: 0.032720, acc.: 100.00%] [G loss: 3.471725]\n",
            "49 [D loss: 0.027158, acc.: 100.00%] [G loss: 3.596126]\n",
            "50 [D loss: 0.021834, acc.: 100.00%] [G loss: 3.525276]\n",
            "51 [D loss: 0.022452, acc.: 100.00%] [G loss: 3.571334]\n",
            "52 [D loss: 0.025386, acc.: 100.00%] [G loss: 3.575841]\n",
            "53 [D loss: 0.024143, acc.: 100.00%] [G loss: 3.554588]\n",
            "54 [D loss: 0.023689, acc.: 100.00%] [G loss: 3.627738]\n",
            "55 [D loss: 0.024383, acc.: 100.00%] [G loss: 3.642354]\n",
            "56 [D loss: 0.024119, acc.: 100.00%] [G loss: 3.631329]\n",
            "57 [D loss: 0.028932, acc.: 100.00%] [G loss: 3.722155]\n",
            "58 [D loss: 0.023633, acc.: 100.00%] [G loss: 3.740204]\n",
            "59 [D loss: 0.024936, acc.: 100.00%] [G loss: 3.838330]\n",
            "60 [D loss: 0.021364, acc.: 100.00%] [G loss: 3.812416]\n",
            "61 [D loss: 0.025884, acc.: 100.00%] [G loss: 3.831615]\n",
            "62 [D loss: 0.025795, acc.: 100.00%] [G loss: 3.897555]\n",
            "63 [D loss: 0.024790, acc.: 100.00%] [G loss: 3.901192]\n",
            "64 [D loss: 0.024387, acc.: 100.00%] [G loss: 3.876535]\n",
            "65 [D loss: 0.026564, acc.: 100.00%] [G loss: 3.889625]\n",
            "66 [D loss: 0.026915, acc.: 100.00%] [G loss: 3.978505]\n",
            "67 [D loss: 0.025725, acc.: 100.00%] [G loss: 4.002016]\n",
            "68 [D loss: 0.022359, acc.: 100.00%] [G loss: 3.907622]\n",
            "69 [D loss: 0.023371, acc.: 100.00%] [G loss: 3.935483]\n",
            "70 [D loss: 0.019351, acc.: 100.00%] [G loss: 4.042576]\n",
            "71 [D loss: 0.025806, acc.: 100.00%] [G loss: 3.908703]\n",
            "72 [D loss: 0.021961, acc.: 100.00%] [G loss: 4.017946]\n",
            "73 [D loss: 0.030027, acc.: 100.00%] [G loss: 4.020434]\n",
            "74 [D loss: 0.024754, acc.: 100.00%] [G loss: 4.030374]\n",
            "75 [D loss: 0.026911, acc.: 100.00%] [G loss: 4.046053]\n",
            "76 [D loss: 0.044565, acc.: 99.62%] [G loss: 4.107772]\n",
            "77 [D loss: 0.034354, acc.: 100.00%] [G loss: 4.172814]\n",
            "78 [D loss: 0.046962, acc.: 99.62%] [G loss: 4.080728]\n",
            "79 [D loss: 0.034988, acc.: 99.62%] [G loss: 4.228458]\n",
            "80 [D loss: 0.051771, acc.: 99.62%] [G loss: 4.143735]\n",
            "81 [D loss: 0.023811, acc.: 100.00%] [G loss: 4.183875]\n",
            "82 [D loss: 0.039501, acc.: 100.00%] [G loss: 4.205180]\n",
            "83 [D loss: 0.094502, acc.: 95.45%] [G loss: 4.312510]\n",
            "84 [D loss: 0.050174, acc.: 98.86%] [G loss: 4.253700]\n",
            "85 [D loss: 0.096915, acc.: 97.35%] [G loss: 3.979860]\n",
            "86 [D loss: 0.055673, acc.: 98.48%] [G loss: 4.439325]\n",
            "87 [D loss: 0.300440, acc.: 88.64%] [G loss: 3.756496]\n",
            "88 [D loss: 0.048694, acc.: 98.48%] [G loss: 4.235838]\n",
            "89 [D loss: 0.037764, acc.: 98.86%] [G loss: 4.297839]\n",
            "90 [D loss: 0.146319, acc.: 93.94%] [G loss: 3.867979]\n",
            "91 [D loss: 0.056467, acc.: 98.86%] [G loss: 4.411649]\n",
            "92 [D loss: 0.329423, acc.: 89.77%] [G loss: 3.310642]\n",
            "93 [D loss: 0.139467, acc.: 93.94%] [G loss: 3.829562]\n",
            "94 [D loss: 0.059416, acc.: 98.86%] [G loss: 4.301535]\n",
            "95 [D loss: 0.204919, acc.: 92.05%] [G loss: 3.561247]\n",
            "96 [D loss: 0.074809, acc.: 96.97%] [G loss: 3.684198]\n",
            "97 [D loss: 0.072645, acc.: 97.73%] [G loss: 3.906221]\n",
            "98 [D loss: 0.497135, acc.: 80.68%] [G loss: 3.137733]\n",
            "99 [D loss: 0.109491, acc.: 94.70%] [G loss: 3.812938]\n",
            "100 [D loss: 0.102752, acc.: 98.11%] [G loss: 3.827116]\n",
            "101 [D loss: 0.262428, acc.: 87.50%] [G loss: 3.234016]\n",
            "102 [D loss: 0.135863, acc.: 94.70%] [G loss: 3.666730]\n",
            "103 [D loss: 0.489265, acc.: 81.44%] [G loss: 2.705248]\n",
            "104 [D loss: 0.158363, acc.: 93.56%] [G loss: 3.303459]\n",
            "105 [D loss: 0.112242, acc.: 96.97%] [G loss: 3.379014]\n",
            "106 [D loss: 0.455236, acc.: 79.17%] [G loss: 2.873304]\n",
            "107 [D loss: 0.110892, acc.: 95.83%] [G loss: 3.437758]\n",
            "108 [D loss: 0.306301, acc.: 87.12%] [G loss: 2.654832]\n",
            "109 [D loss: 0.155675, acc.: 92.05%] [G loss: 3.363039]\n",
            "110 [D loss: 0.441219, acc.: 80.30%] [G loss: 2.562332]\n",
            "111 [D loss: 0.164632, acc.: 90.91%] [G loss: 3.205639]\n",
            "112 [D loss: 0.171944, acc.: 94.32%] [G loss: 3.049211]\n",
            "113 [D loss: 0.303363, acc.: 85.61%] [G loss: 2.622346]\n",
            "114 [D loss: 0.159042, acc.: 93.18%] [G loss: 3.215212]\n",
            "115 [D loss: 0.913173, acc.: 64.39%] [G loss: 1.906972]\n",
            "116 [D loss: 0.458897, acc.: 81.06%] [G loss: 2.072865]\n",
            "117 [D loss: 0.128128, acc.: 96.59%] [G loss: 2.878154]\n",
            "118 [D loss: 0.094071, acc.: 98.11%] [G loss: 3.337031]\n",
            "119 [D loss: 0.234371, acc.: 90.91%] [G loss: 2.714621]\n",
            "120 [D loss: 0.150282, acc.: 96.59%] [G loss: 2.979097]\n",
            "121 [D loss: 0.267987, acc.: 88.26%] [G loss: 2.378944]\n",
            "122 [D loss: 0.216926, acc.: 88.26%] [G loss: 3.016804]\n",
            "123 [D loss: 0.637336, acc.: 70.08%] [G loss: 1.921219]\n",
            "124 [D loss: 0.268197, acc.: 85.61%] [G loss: 2.454741]\n",
            "125 [D loss: 0.132283, acc.: 97.73%] [G loss: 3.249326]\n",
            "126 [D loss: 0.523248, acc.: 72.35%] [G loss: 2.104953]\n",
            "127 [D loss: 0.204152, acc.: 90.53%] [G loss: 2.847010]\n",
            "128 [D loss: 0.142907, acc.: 96.21%] [G loss: 2.920490]\n",
            "129 [D loss: 0.561841, acc.: 71.21%] [G loss: 1.955976]\n",
            "130 [D loss: 0.237087, acc.: 86.36%] [G loss: 2.666013]\n",
            "131 [D loss: 0.212149, acc.: 93.94%] [G loss: 2.585852]\n",
            "132 [D loss: 0.318853, acc.: 84.85%] [G loss: 2.677695]\n",
            "133 [D loss: 0.264940, acc.: 87.50%] [G loss: 2.589963]\n",
            "134 [D loss: 0.352576, acc.: 84.09%] [G loss: 2.614476]\n",
            "135 [D loss: 0.243407, acc.: 89.39%] [G loss: 2.863745]\n",
            "136 [D loss: 0.657715, acc.: 65.91%] [G loss: 1.881992]\n",
            "137 [D loss: 0.179905, acc.: 92.05%] [G loss: 2.976759]\n",
            "138 [D loss: 0.621757, acc.: 69.32%] [G loss: 1.906048]\n",
            "139 [D loss: 0.219609, acc.: 91.29%] [G loss: 2.652440]\n",
            "140 [D loss: 0.258488, acc.: 91.67%] [G loss: 2.675952]\n",
            "141 [D loss: 0.315678, acc.: 84.85%] [G loss: 2.602030]\n",
            "142 [D loss: 0.311860, acc.: 85.23%] [G loss: 2.563014]\n",
            "143 [D loss: 0.334415, acc.: 82.58%] [G loss: 2.272175]\n",
            "144 [D loss: 0.277527, acc.: 89.39%] [G loss: 2.601895]\n",
            "145 [D loss: 0.430551, acc.: 78.41%] [G loss: 2.050155]\n",
            "146 [D loss: 0.233925, acc.: 88.26%] [G loss: 3.057275]\n",
            "147 [D loss: 0.637123, acc.: 66.67%] [G loss: 1.739125]\n",
            "148 [D loss: 0.250978, acc.: 84.85%] [G loss: 3.087456]\n",
            "149 [D loss: 0.294773, acc.: 89.02%] [G loss: 2.699878]\n",
            "150 [D loss: 0.256223, acc.: 87.88%] [G loss: 2.884392]\n",
            "151 [D loss: 0.390085, acc.: 80.30%] [G loss: 2.401139]\n",
            "152 [D loss: 0.286996, acc.: 87.50%] [G loss: 2.767665]\n",
            "153 [D loss: 0.756862, acc.: 62.88%] [G loss: 1.576692]\n",
            "154 [D loss: 0.351891, acc.: 78.41%] [G loss: 2.938665]\n",
            "155 [D loss: 0.523106, acc.: 71.59%] [G loss: 1.924475]\n",
            "156 [D loss: 0.276312, acc.: 88.64%] [G loss: 2.471083]\n",
            "157 [D loss: 0.387408, acc.: 81.06%] [G loss: 2.295758]\n",
            "158 [D loss: 0.353744, acc.: 80.68%] [G loss: 2.588727]\n",
            "159 [D loss: 0.482014, acc.: 73.86%] [G loss: 2.295105]\n",
            "160 [D loss: 0.341356, acc.: 82.58%] [G loss: 2.837773]\n",
            "161 [D loss: 0.617292, acc.: 68.94%] [G loss: 1.723785]\n",
            "162 [D loss: 0.338852, acc.: 79.55%] [G loss: 2.663949]\n",
            "163 [D loss: 0.343212, acc.: 82.58%] [G loss: 2.390656]\n",
            "164 [D loss: 0.300472, acc.: 87.88%] [G loss: 2.559934]\n",
            "165 [D loss: 0.441365, acc.: 74.24%] [G loss: 2.260886]\n",
            "166 [D loss: 0.316521, acc.: 83.71%] [G loss: 2.799881]\n",
            "167 [D loss: 0.528647, acc.: 70.83%] [G loss: 1.833170]\n",
            "168 [D loss: 0.306040, acc.: 84.85%] [G loss: 2.815746]\n",
            "169 [D loss: 0.656968, acc.: 64.02%] [G loss: 1.765350]\n",
            "170 [D loss: 0.277588, acc.: 86.74%] [G loss: 2.885773]\n",
            "171 [D loss: 0.629547, acc.: 66.29%] [G loss: 1.632542]\n",
            "172 [D loss: 0.311325, acc.: 82.95%] [G loss: 2.649602]\n",
            "173 [D loss: 0.380182, acc.: 79.55%] [G loss: 2.527705]\n",
            "174 [D loss: 0.405091, acc.: 78.03%] [G loss: 2.194465]\n",
            "175 [D loss: 0.478353, acc.: 71.59%] [G loss: 2.247543]\n",
            "176 [D loss: 0.379505, acc.: 80.68%] [G loss: 2.448490]\n",
            "177 [D loss: 0.453066, acc.: 77.65%] [G loss: 2.196066]\n",
            "178 [D loss: 0.355708, acc.: 79.17%] [G loss: 2.540865]\n",
            "179 [D loss: 0.576809, acc.: 69.32%] [G loss: 1.907835]\n",
            "180 [D loss: 0.298430, acc.: 83.33%] [G loss: 2.718716]\n",
            "181 [D loss: 0.744128, acc.: 57.58%] [G loss: 1.384035]\n",
            "182 [D loss: 0.336742, acc.: 79.55%] [G loss: 2.674155]\n",
            "183 [D loss: 0.469844, acc.: 75.76%] [G loss: 1.977090]\n",
            "184 [D loss: 0.364320, acc.: 81.44%] [G loss: 2.410934]\n",
            "185 [D loss: 0.376566, acc.: 85.61%] [G loss: 2.193953]\n",
            "186 [D loss: 0.456749, acc.: 76.52%] [G loss: 2.162083]\n",
            "187 [D loss: 0.604939, acc.: 66.67%] [G loss: 1.859951]\n",
            "188 [D loss: 0.408825, acc.: 79.92%] [G loss: 2.342052]\n",
            "189 [D loss: 0.682292, acc.: 60.61%] [G loss: 1.596196]\n",
            "190 [D loss: 0.371982, acc.: 81.06%] [G loss: 2.505756]\n",
            "191 [D loss: 0.862130, acc.: 46.59%] [G loss: 1.157032]\n",
            "192 [D loss: 0.376300, acc.: 76.89%] [G loss: 2.319715]\n",
            "193 [D loss: 0.590671, acc.: 67.05%] [G loss: 1.799334]\n",
            "194 [D loss: 0.401460, acc.: 79.17%] [G loss: 2.176067]\n",
            "195 [D loss: 0.483423, acc.: 73.86%] [G loss: 1.860339]\n",
            "196 [D loss: 0.541516, acc.: 69.70%] [G loss: 2.064247]\n",
            "197 [D loss: 0.591096, acc.: 62.88%] [G loss: 1.766955]\n",
            "198 [D loss: 0.529948, acc.: 70.45%] [G loss: 2.126392]\n",
            "199 [D loss: 0.610945, acc.: 61.74%] [G loss: 1.606956]\n",
            "200 [D loss: 0.537175, acc.: 67.05%] [G loss: 1.956621]\n",
            "201 [D loss: 0.678096, acc.: 54.92%] [G loss: 1.369910]\n",
            "202 [D loss: 0.502504, acc.: 65.53%] [G loss: 2.004877]\n",
            "203 [D loss: 0.877256, acc.: 41.67%] [G loss: 1.106484]\n",
            "204 [D loss: 0.497812, acc.: 65.91%] [G loss: 2.012695]\n",
            "205 [D loss: 0.836827, acc.: 41.67%] [G loss: 1.061904]\n",
            "206 [D loss: 0.513197, acc.: 65.53%] [G loss: 1.836741]\n",
            "207 [D loss: 0.667769, acc.: 56.82%] [G loss: 1.457013]\n",
            "208 [D loss: 0.671035, acc.: 55.30%] [G loss: 1.360049]\n",
            "209 [D loss: 0.630751, acc.: 58.71%] [G loss: 1.485623]\n",
            "210 [D loss: 0.718465, acc.: 47.35%] [G loss: 1.215528]\n",
            "211 [D loss: 0.631383, acc.: 54.92%] [G loss: 1.446989]\n",
            "212 [D loss: 0.792491, acc.: 44.70%] [G loss: 1.061469]\n",
            "213 [D loss: 0.617680, acc.: 59.85%] [G loss: 1.300967]\n",
            "214 [D loss: 0.714345, acc.: 46.97%] [G loss: 1.115009]\n",
            "215 [D loss: 0.679035, acc.: 52.27%] [G loss: 1.151126]\n",
            "216 [D loss: 0.659081, acc.: 58.71%] [G loss: 1.210355]\n",
            "217 [D loss: 0.718640, acc.: 46.97%] [G loss: 0.944281]\n",
            "218 [D loss: 0.684516, acc.: 48.86%] [G loss: 1.047261]\n",
            "219 [D loss: 0.720722, acc.: 45.83%] [G loss: 1.012284]\n",
            "220 [D loss: 0.732412, acc.: 44.32%] [G loss: 0.938647]\n",
            "221 [D loss: 0.699419, acc.: 48.86%] [G loss: 0.957318]\n",
            "222 [D loss: 0.819420, acc.: 38.64%] [G loss: 0.688659]\n",
            "223 [D loss: 0.686904, acc.: 47.35%] [G loss: 0.925105]\n",
            "224 [D loss: 0.799585, acc.: 40.15%] [G loss: 0.754147]\n",
            "225 [D loss: 0.765239, acc.: 42.42%] [G loss: 0.747452]\n",
            "226 [D loss: 0.727898, acc.: 43.94%] [G loss: 0.762228]\n",
            "227 [D loss: 0.708731, acc.: 46.97%] [G loss: 0.762736]\n",
            "228 [D loss: 0.713459, acc.: 47.35%] [G loss: 0.723002]\n",
            "229 [D loss: 0.749503, acc.: 42.05%] [G loss: 0.691250]\n",
            "230 [D loss: 0.693193, acc.: 47.35%] [G loss: 0.754224]\n",
            "231 [D loss: 0.750714, acc.: 39.77%] [G loss: 0.679213]\n",
            "232 [D loss: 0.761377, acc.: 42.05%] [G loss: 0.636948]\n",
            "233 [D loss: 0.721058, acc.: 44.70%] [G loss: 0.643200]\n",
            "234 [D loss: 0.727302, acc.: 45.83%] [G loss: 0.652812]\n",
            "235 [D loss: 0.712319, acc.: 45.45%] [G loss: 0.653954]\n",
            "236 [D loss: 0.708497, acc.: 45.83%] [G loss: 0.648373]\n",
            "237 [D loss: 0.724541, acc.: 44.32%] [G loss: 0.641822]\n",
            "238 [D loss: 0.721590, acc.: 42.80%] [G loss: 0.633125]\n",
            "239 [D loss: 0.718331, acc.: 45.08%] [G loss: 0.619207]\n",
            "240 [D loss: 0.718898, acc.: 45.45%] [G loss: 0.613546]\n",
            "241 [D loss: 0.713953, acc.: 45.08%] [G loss: 0.624696]\n",
            "242 [D loss: 0.698620, acc.: 46.21%] [G loss: 0.633427]\n",
            "243 [D loss: 0.708251, acc.: 45.45%] [G loss: 0.635819]\n",
            "244 [D loss: 0.723916, acc.: 46.21%] [G loss: 0.630312]\n",
            "245 [D loss: 0.713724, acc.: 45.45%] [G loss: 0.608953]\n",
            "246 [D loss: 0.705450, acc.: 48.11%] [G loss: 0.618800]\n",
            "247 [D loss: 0.694226, acc.: 47.35%] [G loss: 0.625266]\n",
            "248 [D loss: 0.698919, acc.: 48.11%] [G loss: 0.637360]\n",
            "249 [D loss: 0.691118, acc.: 47.73%] [G loss: 0.634682]\n",
            "250 [D loss: 0.695242, acc.: 46.59%] [G loss: 0.626336]\n",
            "251 [D loss: 0.682796, acc.: 48.86%] [G loss: 0.639556]\n",
            "252 [D loss: 0.679249, acc.: 48.11%] [G loss: 0.644841]\n",
            "253 [D loss: 0.697187, acc.: 45.45%] [G loss: 0.628311]\n",
            "254 [D loss: 0.696118, acc.: 45.83%] [G loss: 0.628805]\n",
            "255 [D loss: 0.694350, acc.: 46.59%] [G loss: 0.626827]\n",
            "256 [D loss: 0.695072, acc.: 46.21%] [G loss: 0.622103]\n",
            "257 [D loss: 0.677542, acc.: 48.11%] [G loss: 0.625387]\n",
            "258 [D loss: 0.677252, acc.: 47.35%] [G loss: 0.632564]\n",
            "259 [D loss: 0.680856, acc.: 47.73%] [G loss: 0.633260]\n",
            "260 [D loss: 0.674872, acc.: 46.59%] [G loss: 0.634992]\n",
            "261 [D loss: 0.668072, acc.: 48.48%] [G loss: 0.636940]\n",
            "262 [D loss: 0.682926, acc.: 48.11%] [G loss: 0.637930]\n",
            "263 [D loss: 0.680725, acc.: 46.97%] [G loss: 0.629147]\n",
            "264 [D loss: 0.678942, acc.: 47.73%] [G loss: 0.634912]\n",
            "265 [D loss: 0.681208, acc.: 48.11%] [G loss: 0.633828]\n",
            "266 [D loss: 0.670488, acc.: 48.86%] [G loss: 0.627300]\n",
            "267 [D loss: 0.674417, acc.: 46.97%] [G loss: 0.629380]\n",
            "268 [D loss: 0.674905, acc.: 48.48%] [G loss: 0.618051]\n",
            "269 [D loss: 0.684168, acc.: 48.48%] [G loss: 0.613226]\n",
            "270 [D loss: 0.673199, acc.: 46.97%] [G loss: 0.611533]\n",
            "271 [D loss: 0.662316, acc.: 48.86%] [G loss: 0.622351]\n",
            "272 [D loss: 0.665071, acc.: 48.86%] [G loss: 0.627304]\n",
            "273 [D loss: 0.672612, acc.: 47.73%] [G loss: 0.622867]\n",
            "274 [D loss: 0.668664, acc.: 48.11%] [G loss: 0.624407]\n",
            "275 [D loss: 0.664724, acc.: 48.86%] [G loss: 0.624783]\n",
            "276 [D loss: 0.669074, acc.: 48.86%] [G loss: 0.624884]\n",
            "277 [D loss: 0.656477, acc.: 49.24%] [G loss: 0.629290]\n",
            "278 [D loss: 0.663096, acc.: 48.11%] [G loss: 0.624776]\n",
            "279 [D loss: 0.662926, acc.: 48.86%] [G loss: 0.630179]\n",
            "280 [D loss: 0.659023, acc.: 48.86%] [G loss: 0.632321]\n",
            "281 [D loss: 0.654031, acc.: 48.48%] [G loss: 0.631003]\n",
            "282 [D loss: 0.655955, acc.: 49.24%] [G loss: 0.635719]\n",
            "283 [D loss: 0.656172, acc.: 48.48%] [G loss: 0.639422]\n",
            "284 [D loss: 0.674523, acc.: 46.97%] [G loss: 0.641049]\n",
            "285 [D loss: 0.679534, acc.: 48.11%] [G loss: 0.635376]\n",
            "286 [D loss: 0.679668, acc.: 48.11%] [G loss: 0.630707]\n",
            "287 [D loss: 0.680774, acc.: 48.11%] [G loss: 0.635662]\n",
            "288 [D loss: 0.678078, acc.: 48.11%] [G loss: 0.635428]\n",
            "289 [D loss: 0.681088, acc.: 48.48%] [G loss: 0.634312]\n",
            "290 [D loss: 0.679247, acc.: 47.73%] [G loss: 0.635775]\n",
            "291 [D loss: 0.674941, acc.: 48.11%] [G loss: 0.628348]\n",
            "292 [D loss: 0.679419, acc.: 48.86%] [G loss: 0.630946]\n",
            "293 [D loss: 0.669850, acc.: 50.00%] [G loss: 0.639416]\n",
            "294 [D loss: 0.679867, acc.: 49.24%] [G loss: 0.631281]\n",
            "295 [D loss: 0.680604, acc.: 48.86%] [G loss: 0.633808]\n",
            "296 [D loss: 0.684816, acc.: 48.86%] [G loss: 0.635130]\n",
            "297 [D loss: 0.676084, acc.: 49.24%] [G loss: 0.636789]\n",
            "298 [D loss: 0.679145, acc.: 47.35%] [G loss: 0.646783]\n",
            "299 [D loss: 0.671673, acc.: 49.62%] [G loss: 0.646820]\n",
            "300 [D loss: 0.682492, acc.: 48.86%] [G loss: 0.632694]\n",
            "301 [D loss: 0.673778, acc.: 50.38%] [G loss: 0.628128]\n",
            "302 [D loss: 0.680330, acc.: 48.48%] [G loss: 0.622751]\n",
            "303 [D loss: 0.668905, acc.: 48.86%] [G loss: 0.626234]\n",
            "304 [D loss: 0.675779, acc.: 49.24%] [G loss: 0.623461]\n",
            "305 [D loss: 0.667431, acc.: 49.24%] [G loss: 0.630099]\n",
            "306 [D loss: 0.661231, acc.: 49.62%] [G loss: 0.639749]\n",
            "307 [D loss: 0.665046, acc.: 49.62%] [G loss: 0.636467]\n",
            "308 [D loss: 0.661902, acc.: 50.00%] [G loss: 0.639486]\n",
            "309 [D loss: 0.656603, acc.: 50.76%] [G loss: 0.645407]\n",
            "310 [D loss: 0.665298, acc.: 49.24%] [G loss: 0.639063]\n",
            "311 [D loss: 0.662136, acc.: 49.24%] [G loss: 0.635509]\n",
            "312 [D loss: 0.659388, acc.: 48.86%] [G loss: 0.634080]\n",
            "313 [D loss: 0.663871, acc.: 49.62%] [G loss: 0.631829]\n",
            "314 [D loss: 0.655456, acc.: 49.62%] [G loss: 0.638169]\n",
            "315 [D loss: 0.643451, acc.: 50.00%] [G loss: 0.637099]\n",
            "316 [D loss: 0.661649, acc.: 49.62%] [G loss: 0.635726]\n",
            "317 [D loss: 0.648544, acc.: 50.00%] [G loss: 0.641925]\n",
            "318 [D loss: 0.647207, acc.: 49.24%] [G loss: 0.649248]\n",
            "319 [D loss: 0.652916, acc.: 48.48%] [G loss: 0.648972]\n",
            "320 [D loss: 0.652013, acc.: 49.62%] [G loss: 0.646662]\n",
            "321 [D loss: 0.657021, acc.: 48.86%] [G loss: 0.649703]\n",
            "322 [D loss: 0.646689, acc.: 48.86%] [G loss: 0.653898]\n",
            "323 [D loss: 0.648102, acc.: 50.00%] [G loss: 0.653424]\n",
            "324 [D loss: 0.654429, acc.: 48.48%] [G loss: 0.654907]\n",
            "325 [D loss: 0.643967, acc.: 49.62%] [G loss: 0.661491]\n",
            "326 [D loss: 0.655441, acc.: 49.24%] [G loss: 0.664146]\n",
            "327 [D loss: 0.656999, acc.: 48.86%] [G loss: 0.663139]\n",
            "328 [D loss: 0.652089, acc.: 48.86%] [G loss: 0.659994]\n",
            "329 [D loss: 0.654258, acc.: 48.86%] [G loss: 0.655011]\n",
            "330 [D loss: 0.653658, acc.: 50.00%] [G loss: 0.652467]\n",
            "331 [D loss: 0.657203, acc.: 49.24%] [G loss: 0.655755]\n",
            "332 [D loss: 0.656805, acc.: 49.24%] [G loss: 0.646835]\n",
            "333 [D loss: 0.660212, acc.: 49.62%] [G loss: 0.646864]\n",
            "334 [D loss: 0.656725, acc.: 49.62%] [G loss: 0.648942]\n",
            "335 [D loss: 0.649436, acc.: 49.62%] [G loss: 0.653234]\n",
            "336 [D loss: 0.652310, acc.: 50.00%] [G loss: 0.647883]\n",
            "337 [D loss: 0.663683, acc.: 49.24%] [G loss: 0.650555]\n",
            "338 [D loss: 0.648591, acc.: 49.24%] [G loss: 0.655328]\n",
            "339 [D loss: 0.660712, acc.: 48.86%] [G loss: 0.659778]\n",
            "340 [D loss: 0.657578, acc.: 49.62%] [G loss: 0.656578]\n",
            "341 [D loss: 0.654954, acc.: 49.24%] [G loss: 0.661581]\n",
            "342 [D loss: 0.653077, acc.: 48.86%] [G loss: 0.661512]\n",
            "343 [D loss: 0.652427, acc.: 49.62%] [G loss: 0.657960]\n",
            "344 [D loss: 0.644623, acc.: 49.62%] [G loss: 0.657137]\n",
            "345 [D loss: 0.652946, acc.: 49.62%] [G loss: 0.658292]\n",
            "346 [D loss: 0.662431, acc.: 48.48%] [G loss: 0.652528]\n",
            "347 [D loss: 0.654039, acc.: 48.86%] [G loss: 0.647408]\n",
            "348 [D loss: 0.651152, acc.: 49.24%] [G loss: 0.651060]\n",
            "349 [D loss: 0.648262, acc.: 49.24%] [G loss: 0.652667]\n",
            "350 [D loss: 0.662878, acc.: 48.86%] [G loss: 0.654242]\n",
            "351 [D loss: 0.660978, acc.: 48.86%] [G loss: 0.658934]\n",
            "352 [D loss: 0.649585, acc.: 49.62%] [G loss: 0.664333]\n",
            "353 [D loss: 0.656849, acc.: 48.48%] [G loss: 0.660014]\n",
            "354 [D loss: 0.647628, acc.: 49.62%] [G loss: 0.661085]\n",
            "355 [D loss: 0.663016, acc.: 49.62%] [G loss: 0.660889]\n",
            "356 [D loss: 0.663169, acc.: 48.11%] [G loss: 0.659701]\n",
            "357 [D loss: 0.647274, acc.: 50.00%] [G loss: 0.660851]\n",
            "358 [D loss: 0.651995, acc.: 48.86%] [G loss: 0.664935]\n",
            "359 [D loss: 0.651754, acc.: 48.86%] [G loss: 0.662659]\n",
            "360 [D loss: 0.655192, acc.: 48.86%] [G loss: 0.655807]\n",
            "361 [D loss: 0.666925, acc.: 47.73%] [G loss: 0.655658]\n",
            "362 [D loss: 0.657088, acc.: 48.48%] [G loss: 0.651196]\n",
            "363 [D loss: 0.644065, acc.: 49.24%] [G loss: 0.653950]\n",
            "364 [D loss: 0.653863, acc.: 48.48%] [G loss: 0.655482]\n",
            "365 [D loss: 0.658107, acc.: 47.73%] [G loss: 0.660652]\n",
            "366 [D loss: 0.651595, acc.: 48.11%] [G loss: 0.666569]\n",
            "367 [D loss: 0.663810, acc.: 46.59%] [G loss: 0.665706]\n",
            "368 [D loss: 0.663224, acc.: 46.97%] [G loss: 0.664834]\n",
            "369 [D loss: 0.672034, acc.: 46.97%] [G loss: 0.658087]\n",
            "370 [D loss: 0.664747, acc.: 47.73%] [G loss: 0.659256]\n",
            "371 [D loss: 0.656331, acc.: 49.24%] [G loss: 0.664229]\n",
            "372 [D loss: 0.661867, acc.: 48.48%] [G loss: 0.658850]\n",
            "373 [D loss: 0.649889, acc.: 48.86%] [G loss: 0.655185]\n",
            "374 [D loss: 0.649739, acc.: 49.62%] [G loss: 0.660881]\n",
            "375 [D loss: 0.652557, acc.: 49.62%] [G loss: 0.660435]\n",
            "376 [D loss: 0.645851, acc.: 48.86%] [G loss: 0.667596]\n",
            "377 [D loss: 0.647160, acc.: 48.48%] [G loss: 0.665235]\n",
            "378 [D loss: 0.650502, acc.: 49.24%] [G loss: 0.668397]\n",
            "379 [D loss: 0.659381, acc.: 48.48%] [G loss: 0.668692]\n",
            "380 [D loss: 0.659064, acc.: 49.62%] [G loss: 0.668682]\n",
            "381 [D loss: 0.653294, acc.: 49.62%] [G loss: 0.669831]\n",
            "382 [D loss: 0.655777, acc.: 48.48%] [G loss: 0.668601]\n",
            "383 [D loss: 0.647582, acc.: 49.24%] [G loss: 0.664321]\n",
            "384 [D loss: 0.658713, acc.: 48.86%] [G loss: 0.666384]\n",
            "385 [D loss: 0.651212, acc.: 48.86%] [G loss: 0.664681]\n",
            "386 [D loss: 0.658055, acc.: 50.00%] [G loss: 0.659469]\n",
            "387 [D loss: 0.660654, acc.: 50.38%] [G loss: 0.658680]\n",
            "388 [D loss: 0.654332, acc.: 48.48%] [G loss: 0.654253]\n",
            "389 [D loss: 0.652979, acc.: 50.00%] [G loss: 0.662621]\n",
            "390 [D loss: 0.655081, acc.: 48.86%] [G loss: 0.660766]\n",
            "391 [D loss: 0.657063, acc.: 50.38%] [G loss: 0.661473]\n",
            "392 [D loss: 0.662350, acc.: 47.73%] [G loss: 0.660644]\n",
            "393 [D loss: 0.660679, acc.: 48.86%] [G loss: 0.651586]\n",
            "394 [D loss: 0.651457, acc.: 49.24%] [G loss: 0.654641]\n",
            "395 [D loss: 0.665359, acc.: 48.48%] [G loss: 0.643677]\n",
            "396 [D loss: 0.668814, acc.: 49.24%] [G loss: 0.650037]\n",
            "397 [D loss: 0.670539, acc.: 48.86%] [G loss: 0.653809]\n",
            "398 [D loss: 0.663485, acc.: 50.00%] [G loss: 0.661376]\n",
            "399 [D loss: 0.668159, acc.: 50.00%] [G loss: 0.657674]\n",
            "400 [D loss: 0.667260, acc.: 51.14%] [G loss: 0.662646]\n",
            "401 [D loss: 0.664580, acc.: 48.86%] [G loss: 0.662127]\n",
            "402 [D loss: 0.655064, acc.: 51.52%] [G loss: 0.658796]\n",
            "403 [D loss: 0.669676, acc.: 51.52%] [G loss: 0.658331]\n",
            "404 [D loss: 0.666952, acc.: 51.14%] [G loss: 0.659712]\n",
            "405 [D loss: 0.660688, acc.: 50.38%] [G loss: 0.663530]\n",
            "406 [D loss: 0.659754, acc.: 49.62%] [G loss: 0.664648]\n",
            "407 [D loss: 0.659945, acc.: 50.00%] [G loss: 0.672513]\n",
            "408 [D loss: 0.661437, acc.: 48.86%] [G loss: 0.667955]\n",
            "409 [D loss: 0.658965, acc.: 51.14%] [G loss: 0.666951]\n",
            "410 [D loss: 0.669010, acc.: 49.24%] [G loss: 0.664212]\n",
            "411 [D loss: 0.667853, acc.: 49.62%] [G loss: 0.665269]\n",
            "412 [D loss: 0.660978, acc.: 50.00%] [G loss: 0.660514]\n",
            "413 [D loss: 0.666209, acc.: 49.24%] [G loss: 0.651822]\n",
            "414 [D loss: 0.669154, acc.: 49.24%] [G loss: 0.649002]\n",
            "415 [D loss: 0.669685, acc.: 49.62%] [G loss: 0.649339]\n",
            "416 [D loss: 0.671662, acc.: 49.62%] [G loss: 0.661625]\n",
            "417 [D loss: 0.665327, acc.: 49.62%] [G loss: 0.659294]\n",
            "418 [D loss: 0.666921, acc.: 50.00%] [G loss: 0.665745]\n",
            "419 [D loss: 0.673409, acc.: 49.62%] [G loss: 0.664224]\n",
            "420 [D loss: 0.679132, acc.: 48.86%] [G loss: 0.656666]\n",
            "421 [D loss: 0.663191, acc.: 49.62%] [G loss: 0.658918]\n",
            "422 [D loss: 0.656319, acc.: 50.00%] [G loss: 0.660238]\n",
            "423 [D loss: 0.658033, acc.: 50.38%] [G loss: 0.661615]\n",
            "424 [D loss: 0.657115, acc.: 49.62%] [G loss: 0.674682]\n",
            "425 [D loss: 0.659024, acc.: 50.38%] [G loss: 0.672365]\n",
            "426 [D loss: 0.656267, acc.: 50.00%] [G loss: 0.668453]\n",
            "427 [D loss: 0.659627, acc.: 51.14%] [G loss: 0.662657]\n",
            "428 [D loss: 0.658144, acc.: 49.62%] [G loss: 0.656322]\n",
            "429 [D loss: 0.656916, acc.: 50.38%] [G loss: 0.656817]\n",
            "430 [D loss: 0.657540, acc.: 50.38%] [G loss: 0.657590]\n",
            "431 [D loss: 0.662854, acc.: 50.00%] [G loss: 0.652153]\n",
            "432 [D loss: 0.659425, acc.: 49.24%] [G loss: 0.656228]\n",
            "433 [D loss: 0.657900, acc.: 49.62%] [G loss: 0.658991]\n",
            "434 [D loss: 0.660926, acc.: 50.00%] [G loss: 0.657207]\n",
            "435 [D loss: 0.657801, acc.: 50.00%] [G loss: 0.658054]\n",
            "436 [D loss: 0.663702, acc.: 50.00%] [G loss: 0.657989]\n",
            "437 [D loss: 0.657988, acc.: 50.00%] [G loss: 0.662004]\n",
            "438 [D loss: 0.660011, acc.: 50.38%] [G loss: 0.662968]\n",
            "439 [D loss: 0.656884, acc.: 50.00%] [G loss: 0.659481]\n",
            "440 [D loss: 0.653785, acc.: 50.76%] [G loss: 0.661685]\n",
            "441 [D loss: 0.659104, acc.: 50.00%] [G loss: 0.664525]\n",
            "442 [D loss: 0.648341, acc.: 50.00%] [G loss: 0.663282]\n",
            "443 [D loss: 0.650539, acc.: 51.14%] [G loss: 0.668438]\n",
            "444 [D loss: 0.663974, acc.: 50.38%] [G loss: 0.678925]\n",
            "445 [D loss: 0.656951, acc.: 50.00%] [G loss: 0.677223]\n",
            "446 [D loss: 0.649894, acc.: 50.76%] [G loss: 0.677345]\n",
            "447 [D loss: 0.654990, acc.: 50.00%] [G loss: 0.676143]\n",
            "448 [D loss: 0.658968, acc.: 49.24%] [G loss: 0.672336]\n",
            "449 [D loss: 0.651246, acc.: 50.00%] [G loss: 0.672042]\n",
            "450 [D loss: 0.647117, acc.: 50.00%] [G loss: 0.665973]\n",
            "451 [D loss: 0.660729, acc.: 48.48%] [G loss: 0.661821]\n",
            "452 [D loss: 0.658378, acc.: 49.24%] [G loss: 0.654049]\n",
            "453 [D loss: 0.651191, acc.: 50.00%] [G loss: 0.646953]\n",
            "454 [D loss: 0.661836, acc.: 50.00%] [G loss: 0.652037]\n",
            "455 [D loss: 0.651583, acc.: 50.00%] [G loss: 0.661490]\n",
            "456 [D loss: 0.655365, acc.: 50.76%] [G loss: 0.671655]\n",
            "457 [D loss: 0.656006, acc.: 50.00%] [G loss: 0.667608]\n",
            "458 [D loss: 0.646178, acc.: 50.00%] [G loss: 0.662633]\n",
            "459 [D loss: 0.655811, acc.: 50.38%] [G loss: 0.666019]\n",
            "460 [D loss: 0.660583, acc.: 50.00%] [G loss: 0.665806]\n",
            "461 [D loss: 0.658196, acc.: 50.76%] [G loss: 0.666028]\n",
            "462 [D loss: 0.652515, acc.: 51.52%] [G loss: 0.668480]\n",
            "463 [D loss: 0.652714, acc.: 51.14%] [G loss: 0.668510]\n",
            "464 [D loss: 0.654243, acc.: 51.14%] [G loss: 0.664248]\n",
            "465 [D loss: 0.654218, acc.: 51.52%] [G loss: 0.663474]\n",
            "466 [D loss: 0.653990, acc.: 51.89%] [G loss: 0.666892]\n",
            "467 [D loss: 0.647892, acc.: 51.14%] [G loss: 0.664645]\n",
            "468 [D loss: 0.646801, acc.: 52.27%] [G loss: 0.668082]\n",
            "469 [D loss: 0.654290, acc.: 53.79%] [G loss: 0.673653]\n",
            "470 [D loss: 0.650656, acc.: 50.76%] [G loss: 0.666968]\n",
            "471 [D loss: 0.648620, acc.: 51.89%] [G loss: 0.669421]\n",
            "472 [D loss: 0.647565, acc.: 54.17%] [G loss: 0.667793]\n",
            "473 [D loss: 0.649036, acc.: 52.27%] [G loss: 0.673444]\n",
            "474 [D loss: 0.645625, acc.: 53.79%] [G loss: 0.674527]\n",
            "475 [D loss: 0.649799, acc.: 51.89%] [G loss: 0.668402]\n",
            "476 [D loss: 0.652875, acc.: 55.68%] [G loss: 0.667833]\n",
            "477 [D loss: 0.656582, acc.: 51.89%] [G loss: 0.678674]\n",
            "478 [D loss: 0.638615, acc.: 55.68%] [G loss: 0.686181]\n",
            "479 [D loss: 0.643797, acc.: 53.79%] [G loss: 0.684611]\n",
            "480 [D loss: 0.640311, acc.: 56.82%] [G loss: 0.691876]\n",
            "481 [D loss: 0.650267, acc.: 53.41%] [G loss: 0.685734]\n",
            "482 [D loss: 0.638961, acc.: 55.68%] [G loss: 0.682578]\n",
            "483 [D loss: 0.644534, acc.: 56.44%] [G loss: 0.674051]\n",
            "484 [D loss: 0.649269, acc.: 53.03%] [G loss: 0.678550]\n",
            "485 [D loss: 0.645458, acc.: 54.55%] [G loss: 0.676314]\n",
            "486 [D loss: 0.647645, acc.: 55.30%] [G loss: 0.675268]\n",
            "487 [D loss: 0.654242, acc.: 58.71%] [G loss: 0.677248]\n",
            "488 [D loss: 0.647024, acc.: 55.68%] [G loss: 0.684191]\n",
            "489 [D loss: 0.650043, acc.: 57.58%] [G loss: 0.687474]\n",
            "490 [D loss: 0.644746, acc.: 57.20%] [G loss: 0.689972]\n",
            "491 [D loss: 0.653184, acc.: 55.30%] [G loss: 0.690522]\n",
            "492 [D loss: 0.654170, acc.: 54.17%] [G loss: 0.692990]\n",
            "493 [D loss: 0.649761, acc.: 56.82%] [G loss: 0.697509]\n",
            "494 [D loss: 0.648800, acc.: 53.41%] [G loss: 0.692042]\n",
            "495 [D loss: 0.649602, acc.: 54.92%] [G loss: 0.700051]\n",
            "496 [D loss: 0.655461, acc.: 50.00%] [G loss: 0.702855]\n",
            "497 [D loss: 0.655448, acc.: 51.89%] [G loss: 0.703017]\n",
            "498 [D loss: 0.651279, acc.: 53.03%] [G loss: 0.698495]\n",
            "499 [D loss: 0.655700, acc.: 50.38%] [G loss: 0.700134]\n",
            "500 [D loss: 0.647556, acc.: 52.65%] [G loss: 0.700048]\n",
            "501 [D loss: 0.651257, acc.: 50.38%] [G loss: 0.702538]\n",
            "502 [D loss: 0.644931, acc.: 52.27%] [G loss: 0.702495]\n",
            "503 [D loss: 0.650483, acc.: 49.24%] [G loss: 0.715792]\n",
            "504 [D loss: 0.637018, acc.: 51.52%] [G loss: 0.715178]\n",
            "505 [D loss: 0.645155, acc.: 50.00%] [G loss: 0.703049]\n",
            "506 [D loss: 0.641576, acc.: 49.62%] [G loss: 0.698063]\n",
            "507 [D loss: 0.644531, acc.: 51.89%] [G loss: 0.696376]\n",
            "508 [D loss: 0.643368, acc.: 48.86%] [G loss: 0.695199]\n",
            "509 [D loss: 0.639204, acc.: 51.89%] [G loss: 0.697188]\n",
            "510 [D loss: 0.629936, acc.: 51.14%] [G loss: 0.696489]\n",
            "511 [D loss: 0.632962, acc.: 49.62%] [G loss: 0.696020]\n",
            "512 [D loss: 0.633599, acc.: 51.89%] [G loss: 0.696685]\n",
            "513 [D loss: 0.640950, acc.: 48.86%] [G loss: 0.692406]\n",
            "514 [D loss: 0.631375, acc.: 51.14%] [G loss: 0.696415]\n",
            "515 [D loss: 0.628904, acc.: 51.89%] [G loss: 0.695026]\n",
            "516 [D loss: 0.631882, acc.: 51.89%] [G loss: 0.690112]\n",
            "517 [D loss: 0.637447, acc.: 51.52%] [G loss: 0.694236]\n",
            "518 [D loss: 0.631310, acc.: 51.14%] [G loss: 0.697561]\n",
            "519 [D loss: 0.625600, acc.: 56.06%] [G loss: 0.698934]\n",
            "520 [D loss: 0.633798, acc.: 53.03%] [G loss: 0.696484]\n",
            "521 [D loss: 0.631176, acc.: 51.52%] [G loss: 0.701408]\n",
            "522 [D loss: 0.620090, acc.: 53.03%] [G loss: 0.711882]\n",
            "523 [D loss: 0.628886, acc.: 54.17%] [G loss: 0.709131]\n",
            "524 [D loss: 0.634882, acc.: 53.03%] [G loss: 0.704123]\n",
            "525 [D loss: 0.624802, acc.: 53.79%] [G loss: 0.696625]\n",
            "526 [D loss: 0.634078, acc.: 55.30%] [G loss: 0.703373]\n",
            "527 [D loss: 0.624805, acc.: 54.92%] [G loss: 0.705400]\n",
            "528 [D loss: 0.638267, acc.: 54.55%] [G loss: 0.704497]\n",
            "529 [D loss: 0.624442, acc.: 57.20%] [G loss: 0.700352]\n",
            "530 [D loss: 0.642769, acc.: 53.79%] [G loss: 0.706886]\n",
            "531 [D loss: 0.636085, acc.: 60.23%] [G loss: 0.707273]\n",
            "532 [D loss: 0.634724, acc.: 62.12%] [G loss: 0.719536]\n",
            "533 [D loss: 0.650153, acc.: 57.20%] [G loss: 0.703986]\n",
            "534 [D loss: 0.637110, acc.: 60.61%] [G loss: 0.712963]\n",
            "535 [D loss: 0.645561, acc.: 55.30%] [G loss: 0.712239]\n",
            "536 [D loss: 0.640867, acc.: 55.30%] [G loss: 0.711275]\n",
            "537 [D loss: 0.643665, acc.: 52.65%] [G loss: 0.707144]\n",
            "538 [D loss: 0.650603, acc.: 57.20%] [G loss: 0.720606]\n",
            "539 [D loss: 0.644336, acc.: 53.03%] [G loss: 0.719423]\n",
            "540 [D loss: 0.651708, acc.: 55.30%] [G loss: 0.725581]\n",
            "541 [D loss: 0.646557, acc.: 54.17%] [G loss: 0.744203]\n",
            "542 [D loss: 0.654320, acc.: 53.03%] [G loss: 0.733495]\n",
            "543 [D loss: 0.649875, acc.: 56.06%] [G loss: 0.732925]\n",
            "544 [D loss: 0.646072, acc.: 57.20%] [G loss: 0.719228]\n",
            "545 [D loss: 0.655695, acc.: 54.55%] [G loss: 0.708622]\n",
            "546 [D loss: 0.652038, acc.: 59.47%] [G loss: 0.711540]\n",
            "547 [D loss: 0.649316, acc.: 56.06%] [G loss: 0.709992]\n",
            "548 [D loss: 0.649454, acc.: 59.85%] [G loss: 0.720344]\n",
            "549 [D loss: 0.639436, acc.: 59.85%] [G loss: 0.727237]\n",
            "550 [D loss: 0.631743, acc.: 60.61%] [G loss: 0.734277]\n",
            "551 [D loss: 0.647035, acc.: 57.95%] [G loss: 0.743884]\n",
            "552 [D loss: 0.623404, acc.: 60.98%] [G loss: 0.746478]\n",
            "553 [D loss: 0.625062, acc.: 57.95%] [G loss: 0.744300]\n",
            "554 [D loss: 0.631214, acc.: 53.03%] [G loss: 0.737928]\n",
            "555 [D loss: 0.628225, acc.: 57.20%] [G loss: 0.730118]\n",
            "556 [D loss: 0.623274, acc.: 56.44%] [G loss: 0.727630]\n",
            "557 [D loss: 0.626364, acc.: 56.06%] [G loss: 0.724126]\n",
            "558 [D loss: 0.633405, acc.: 57.20%] [G loss: 0.722595]\n",
            "559 [D loss: 0.620534, acc.: 59.47%] [G loss: 0.729078]\n",
            "560 [D loss: 0.626430, acc.: 55.68%] [G loss: 0.731152]\n",
            "561 [D loss: 0.629090, acc.: 53.79%] [G loss: 0.741981]\n",
            "562 [D loss: 0.631706, acc.: 56.06%] [G loss: 0.735210]\n",
            "563 [D loss: 0.626712, acc.: 63.26%] [G loss: 0.732268]\n",
            "564 [D loss: 0.639322, acc.: 57.20%] [G loss: 0.732906]\n",
            "565 [D loss: 0.637833, acc.: 59.09%] [G loss: 0.737145]\n",
            "566 [D loss: 0.640949, acc.: 57.20%] [G loss: 0.730571]\n",
            "567 [D loss: 0.642699, acc.: 55.30%] [G loss: 0.733684]\n",
            "568 [D loss: 0.639832, acc.: 56.06%] [G loss: 0.723714]\n",
            "569 [D loss: 0.641859, acc.: 51.89%] [G loss: 0.725537]\n",
            "570 [D loss: 0.636812, acc.: 55.30%] [G loss: 0.739654]\n",
            "571 [D loss: 0.652111, acc.: 52.65%] [G loss: 0.730135]\n",
            "572 [D loss: 0.639114, acc.: 56.06%] [G loss: 0.723654]\n",
            "573 [D loss: 0.643768, acc.: 58.71%] [G loss: 0.724513]\n",
            "574 [D loss: 0.632109, acc.: 60.61%] [G loss: 0.719553]\n",
            "575 [D loss: 0.636167, acc.: 59.85%] [G loss: 0.723084]\n",
            "576 [D loss: 0.641372, acc.: 61.74%] [G loss: 0.743182]\n",
            "577 [D loss: 0.631315, acc.: 61.36%] [G loss: 0.755723]\n",
            "578 [D loss: 0.639798, acc.: 64.02%] [G loss: 0.753918]\n",
            "579 [D loss: 0.643289, acc.: 62.12%] [G loss: 0.748154]\n",
            "580 [D loss: 0.644014, acc.: 57.20%] [G loss: 0.742664]\n",
            "581 [D loss: 0.653452, acc.: 55.30%] [G loss: 0.732668]\n",
            "582 [D loss: 0.648663, acc.: 59.47%] [G loss: 0.745225]\n",
            "583 [D loss: 0.642626, acc.: 60.61%] [G loss: 0.749313]\n",
            "584 [D loss: 0.646844, acc.: 68.56%] [G loss: 0.752488]\n",
            "585 [D loss: 0.639121, acc.: 67.80%] [G loss: 0.740553]\n",
            "586 [D loss: 0.639639, acc.: 62.12%] [G loss: 0.749671]\n",
            "587 [D loss: 0.644800, acc.: 61.74%] [G loss: 0.768983]\n",
            "588 [D loss: 0.652402, acc.: 60.61%] [G loss: 0.785016]\n",
            "589 [D loss: 0.646186, acc.: 58.71%] [G loss: 0.789020]\n",
            "590 [D loss: 0.649966, acc.: 57.20%] [G loss: 0.766823]\n",
            "591 [D loss: 0.641445, acc.: 59.09%] [G loss: 0.758651]\n",
            "592 [D loss: 0.631940, acc.: 55.68%] [G loss: 0.764766]\n",
            "593 [D loss: 0.634845, acc.: 54.17%] [G loss: 0.760341]\n",
            "594 [D loss: 0.640955, acc.: 53.03%] [G loss: 0.749886]\n",
            "595 [D loss: 0.620500, acc.: 57.95%] [G loss: 0.748078]\n",
            "596 [D loss: 0.613436, acc.: 60.23%] [G loss: 0.757578]\n",
            "597 [D loss: 0.614739, acc.: 63.26%] [G loss: 0.766291]\n",
            "598 [D loss: 0.621127, acc.: 60.61%] [G loss: 0.764192]\n",
            "599 [D loss: 0.615997, acc.: 64.02%] [G loss: 0.763787]\n",
            "600 [D loss: 0.616783, acc.: 68.94%] [G loss: 0.774073]\n",
            "601 [D loss: 0.620782, acc.: 65.15%] [G loss: 0.773830]\n",
            "602 [D loss: 0.620322, acc.: 64.02%] [G loss: 0.775318]\n",
            "603 [D loss: 0.635525, acc.: 65.91%] [G loss: 0.779521]\n",
            "604 [D loss: 0.632277, acc.: 63.64%] [G loss: 0.769517]\n",
            "605 [D loss: 0.628502, acc.: 62.50%] [G loss: 0.772892]\n",
            "606 [D loss: 0.629250, acc.: 62.50%] [G loss: 0.775876]\n",
            "607 [D loss: 0.636038, acc.: 63.26%] [G loss: 0.758192]\n",
            "608 [D loss: 0.639436, acc.: 60.61%] [G loss: 0.755915]\n",
            "609 [D loss: 0.641627, acc.: 63.64%] [G loss: 0.756401]\n",
            "610 [D loss: 0.632157, acc.: 65.53%] [G loss: 0.757547]\n",
            "611 [D loss: 0.637820, acc.: 61.36%] [G loss: 0.762596]\n",
            "612 [D loss: 0.636947, acc.: 63.26%] [G loss: 0.766097]\n",
            "613 [D loss: 0.644587, acc.: 60.23%] [G loss: 0.767894]\n",
            "614 [D loss: 0.634558, acc.: 67.05%] [G loss: 0.775366]\n",
            "615 [D loss: 0.625660, acc.: 65.53%] [G loss: 0.779381]\n",
            "616 [D loss: 0.625827, acc.: 64.02%] [G loss: 0.770922]\n",
            "617 [D loss: 0.630182, acc.: 65.53%] [G loss: 0.759871]\n",
            "618 [D loss: 0.623424, acc.: 64.39%] [G loss: 0.768923]\n",
            "619 [D loss: 0.611557, acc.: 66.29%] [G loss: 0.774486]\n",
            "620 [D loss: 0.617588, acc.: 65.15%] [G loss: 0.783649]\n",
            "621 [D loss: 0.617215, acc.: 65.91%] [G loss: 0.784877]\n",
            "622 [D loss: 0.625073, acc.: 68.18%] [G loss: 0.788165]\n",
            "623 [D loss: 0.616022, acc.: 66.29%] [G loss: 0.779230]\n",
            "624 [D loss: 0.610494, acc.: 69.70%] [G loss: 0.782154]\n",
            "625 [D loss: 0.609622, acc.: 73.11%] [G loss: 0.775511]\n",
            "626 [D loss: 0.605670, acc.: 73.86%] [G loss: 0.771144]\n",
            "627 [D loss: 0.600217, acc.: 73.48%] [G loss: 0.780148]\n",
            "628 [D loss: 0.604261, acc.: 71.59%] [G loss: 0.785753]\n",
            "629 [D loss: 0.607159, acc.: 69.32%] [G loss: 0.797700]\n",
            "630 [D loss: 0.606317, acc.: 74.24%] [G loss: 0.797723]\n",
            "631 [D loss: 0.603669, acc.: 75.76%] [G loss: 0.790997]\n",
            "632 [D loss: 0.615080, acc.: 69.70%] [G loss: 0.776180]\n",
            "633 [D loss: 0.604542, acc.: 73.48%] [G loss: 0.761542]\n",
            "634 [D loss: 0.617781, acc.: 68.56%] [G loss: 0.755891]\n",
            "635 [D loss: 0.620865, acc.: 67.42%] [G loss: 0.757877]\n",
            "636 [D loss: 0.625142, acc.: 65.15%] [G loss: 0.755325]\n",
            "637 [D loss: 0.621474, acc.: 67.80%] [G loss: 0.759423]\n",
            "638 [D loss: 0.609394, acc.: 68.18%] [G loss: 0.765210]\n",
            "639 [D loss: 0.621303, acc.: 69.32%] [G loss: 0.771497]\n",
            "640 [D loss: 0.619692, acc.: 71.59%] [G loss: 0.771342]\n",
            "641 [D loss: 0.620275, acc.: 68.18%] [G loss: 0.764637]\n",
            "642 [D loss: 0.617693, acc.: 68.94%] [G loss: 0.755657]\n",
            "643 [D loss: 0.613064, acc.: 69.70%] [G loss: 0.752049]\n",
            "644 [D loss: 0.607074, acc.: 67.05%] [G loss: 0.758573]\n",
            "645 [D loss: 0.616575, acc.: 65.53%] [G loss: 0.789166]\n",
            "646 [D loss: 0.606388, acc.: 70.45%] [G loss: 0.804710]\n",
            "647 [D loss: 0.609789, acc.: 70.08%] [G loss: 0.788609]\n",
            "648 [D loss: 0.608882, acc.: 68.94%] [G loss: 0.769289]\n",
            "649 [D loss: 0.606928, acc.: 65.15%] [G loss: 0.765876]\n",
            "650 [D loss: 0.631340, acc.: 61.74%] [G loss: 0.743870]\n",
            "651 [D loss: 0.624306, acc.: 65.53%] [G loss: 0.756173]\n",
            "652 [D loss: 0.619151, acc.: 61.36%] [G loss: 0.772177]\n",
            "653 [D loss: 0.652002, acc.: 59.47%] [G loss: 0.767938]\n",
            "654 [D loss: 0.636427, acc.: 62.50%] [G loss: 0.762003]\n",
            "655 [D loss: 0.641613, acc.: 62.12%] [G loss: 0.765454]\n",
            "656 [D loss: 0.629766, acc.: 68.18%] [G loss: 0.773786]\n",
            "657 [D loss: 0.636024, acc.: 63.64%] [G loss: 0.794780]\n",
            "658 [D loss: 0.629980, acc.: 64.77%] [G loss: 0.798794]\n",
            "659 [D loss: 0.632080, acc.: 63.26%] [G loss: 0.797567]\n",
            "660 [D loss: 0.633294, acc.: 64.77%] [G loss: 0.812373]\n",
            "661 [D loss: 0.635819, acc.: 60.23%] [G loss: 0.810992]\n",
            "662 [D loss: 0.618645, acc.: 67.42%] [G loss: 0.821980]\n",
            "663 [D loss: 0.631859, acc.: 64.02%] [G loss: 0.805566]\n",
            "664 [D loss: 0.624207, acc.: 60.98%] [G loss: 0.796719]\n",
            "665 [D loss: 0.637504, acc.: 63.64%] [G loss: 0.773042]\n",
            "666 [D loss: 0.650276, acc.: 61.36%] [G loss: 0.752913]\n",
            "667 [D loss: 0.635902, acc.: 60.98%] [G loss: 0.755912]\n",
            "668 [D loss: 0.638084, acc.: 59.85%] [G loss: 0.761199]\n",
            "669 [D loss: 0.636641, acc.: 64.39%] [G loss: 0.758856]\n",
            "670 [D loss: 0.626933, acc.: 61.74%] [G loss: 0.785935]\n",
            "671 [D loss: 0.628959, acc.: 65.91%] [G loss: 0.773753]\n",
            "672 [D loss: 0.628845, acc.: 66.29%] [G loss: 0.764436]\n",
            "673 [D loss: 0.640076, acc.: 60.23%] [G loss: 0.763790]\n",
            "674 [D loss: 0.636421, acc.: 62.88%] [G loss: 0.763655]\n",
            "675 [D loss: 0.631279, acc.: 60.23%] [G loss: 0.776979]\n",
            "676 [D loss: 0.634716, acc.: 60.98%] [G loss: 0.767741]\n",
            "677 [D loss: 0.645564, acc.: 55.68%] [G loss: 0.771350]\n",
            "678 [D loss: 0.632276, acc.: 59.85%] [G loss: 0.772693]\n",
            "679 [D loss: 0.607506, acc.: 68.18%] [G loss: 0.773129]\n",
            "680 [D loss: 0.642229, acc.: 62.50%] [G loss: 0.761633]\n",
            "681 [D loss: 0.636073, acc.: 61.74%] [G loss: 0.775622]\n",
            "682 [D loss: 0.626013, acc.: 62.88%] [G loss: 0.766144]\n",
            "683 [D loss: 0.639671, acc.: 64.02%] [G loss: 0.755131]\n",
            "684 [D loss: 0.649780, acc.: 57.20%] [G loss: 0.748795]\n",
            "685 [D loss: 0.643085, acc.: 62.88%] [G loss: 0.771365]\n",
            "686 [D loss: 0.628678, acc.: 65.15%] [G loss: 0.794488]\n",
            "687 [D loss: 0.619827, acc.: 65.53%] [G loss: 0.794224]\n",
            "688 [D loss: 0.620237, acc.: 65.53%] [G loss: 0.802446]\n",
            "689 [D loss: 0.618355, acc.: 63.26%] [G loss: 0.812707]\n",
            "690 [D loss: 0.619857, acc.: 67.05%] [G loss: 0.791494]\n",
            "691 [D loss: 0.636044, acc.: 63.26%] [G loss: 0.804984]\n",
            "692 [D loss: 0.622723, acc.: 65.91%] [G loss: 0.802628]\n",
            "693 [D loss: 0.617092, acc.: 69.70%] [G loss: 0.798516]\n",
            "694 [D loss: 0.623620, acc.: 65.53%] [G loss: 0.782789]\n",
            "695 [D loss: 0.631983, acc.: 66.29%] [G loss: 0.781329]\n",
            "696 [D loss: 0.618908, acc.: 67.42%] [G loss: 0.782574]\n",
            "697 [D loss: 0.622944, acc.: 70.08%] [G loss: 0.779586]\n",
            "698 [D loss: 0.619975, acc.: 65.91%] [G loss: 0.789290]\n",
            "699 [D loss: 0.616491, acc.: 70.08%] [G loss: 0.798218]\n",
            "700 [D loss: 0.616775, acc.: 66.67%] [G loss: 0.818346]\n",
            "701 [D loss: 0.606056, acc.: 73.11%] [G loss: 0.805279]\n",
            "702 [D loss: 0.602193, acc.: 70.08%] [G loss: 0.791706]\n",
            "703 [D loss: 0.609766, acc.: 70.08%] [G loss: 0.797765]\n",
            "704 [D loss: 0.603371, acc.: 74.24%] [G loss: 0.786679]\n",
            "705 [D loss: 0.605218, acc.: 69.70%] [G loss: 0.813571]\n",
            "706 [D loss: 0.615393, acc.: 71.21%] [G loss: 0.802956]\n",
            "707 [D loss: 0.589400, acc.: 69.70%] [G loss: 0.814257]\n",
            "708 [D loss: 0.617602, acc.: 69.32%] [G loss: 0.805950]\n",
            "709 [D loss: 0.607262, acc.: 68.56%] [G loss: 0.789818]\n",
            "710 [D loss: 0.592578, acc.: 72.35%] [G loss: 0.796536]\n",
            "711 [D loss: 0.604791, acc.: 67.80%] [G loss: 0.795777]\n",
            "712 [D loss: 0.610199, acc.: 66.29%] [G loss: 0.801170]\n",
            "713 [D loss: 0.615738, acc.: 67.42%] [G loss: 0.791102]\n",
            "714 [D loss: 0.608330, acc.: 69.70%] [G loss: 0.803484]\n",
            "715 [D loss: 0.619996, acc.: 65.91%] [G loss: 0.830160]\n",
            "716 [D loss: 0.598117, acc.: 75.00%] [G loss: 0.825443]\n",
            "717 [D loss: 0.623165, acc.: 65.91%] [G loss: 0.803301]\n",
            "718 [D loss: 0.604142, acc.: 67.80%] [G loss: 0.816893]\n",
            "719 [D loss: 0.596044, acc.: 68.94%] [G loss: 0.845985]\n",
            "720 [D loss: 0.591616, acc.: 76.14%] [G loss: 0.862268]\n",
            "721 [D loss: 0.603586, acc.: 76.89%] [G loss: 0.831456]\n",
            "722 [D loss: 0.619598, acc.: 69.32%] [G loss: 0.797133]\n",
            "723 [D loss: 0.594021, acc.: 75.00%] [G loss: 0.813599]\n",
            "724 [D loss: 0.605462, acc.: 73.11%] [G loss: 0.829194]\n",
            "725 [D loss: 0.636261, acc.: 65.15%] [G loss: 0.856597]\n",
            "726 [D loss: 0.609798, acc.: 70.45%] [G loss: 0.876665]\n",
            "727 [D loss: 0.611821, acc.: 68.94%] [G loss: 0.870399]\n",
            "728 [D loss: 0.605089, acc.: 68.18%] [G loss: 0.854649]\n",
            "729 [D loss: 0.606873, acc.: 64.39%] [G loss: 0.840844]\n",
            "730 [D loss: 0.600562, acc.: 70.08%] [G loss: 0.835187]\n",
            "731 [D loss: 0.601106, acc.: 68.56%] [G loss: 0.831279]\n",
            "732 [D loss: 0.628799, acc.: 61.36%] [G loss: 0.797426]\n",
            "733 [D loss: 0.599447, acc.: 69.70%] [G loss: 0.816722]\n",
            "734 [D loss: 0.609409, acc.: 65.15%] [G loss: 0.845638]\n",
            "735 [D loss: 0.583566, acc.: 72.35%] [G loss: 0.877284]\n",
            "736 [D loss: 0.613845, acc.: 66.67%] [G loss: 0.855526]\n",
            "737 [D loss: 0.598550, acc.: 69.32%] [G loss: 0.887601]\n",
            "738 [D loss: 0.618832, acc.: 61.36%] [G loss: 0.878490]\n",
            "739 [D loss: 0.599673, acc.: 69.32%] [G loss: 0.892009]\n",
            "740 [D loss: 0.590073, acc.: 74.62%] [G loss: 0.897869]\n",
            "741 [D loss: 0.619504, acc.: 70.45%] [G loss: 0.884776]\n",
            "742 [D loss: 0.615917, acc.: 64.02%] [G loss: 0.856798]\n",
            "743 [D loss: 0.608134, acc.: 67.80%] [G loss: 0.889590]\n",
            "744 [D loss: 0.611676, acc.: 64.39%] [G loss: 0.885058]\n",
            "745 [D loss: 0.593759, acc.: 70.45%] [G loss: 0.893950]\n",
            "746 [D loss: 0.620951, acc.: 64.02%] [G loss: 0.863555]\n",
            "747 [D loss: 0.610347, acc.: 65.53%] [G loss: 0.858204]\n",
            "748 [D loss: 0.607141, acc.: 60.61%] [G loss: 0.855403]\n",
            "749 [D loss: 0.629327, acc.: 56.82%] [G loss: 0.845976]\n",
            "750 [D loss: 0.633317, acc.: 62.12%] [G loss: 0.839256]\n",
            "751 [D loss: 0.627728, acc.: 60.23%] [G loss: 0.845536]\n",
            "752 [D loss: 0.643211, acc.: 55.30%] [G loss: 0.838728]\n",
            "753 [D loss: 0.652364, acc.: 54.92%] [G loss: 0.807786]\n",
            "754 [D loss: 0.656804, acc.: 50.38%] [G loss: 0.824822]\n",
            "755 [D loss: 0.640440, acc.: 60.23%] [G loss: 0.800670]\n",
            "756 [D loss: 0.643961, acc.: 58.71%] [G loss: 0.791864]\n",
            "757 [D loss: 0.654155, acc.: 57.58%] [G loss: 0.828343]\n",
            "758 [D loss: 0.647239, acc.: 59.09%] [G loss: 0.853518]\n",
            "759 [D loss: 0.655578, acc.: 62.50%] [G loss: 0.832984]\n",
            "760 [D loss: 0.650069, acc.: 56.82%] [G loss: 0.828719]\n",
            "761 [D loss: 0.630357, acc.: 60.98%] [G loss: 0.838813]\n",
            "762 [D loss: 0.636683, acc.: 60.61%] [G loss: 0.829549]\n",
            "763 [D loss: 0.640721, acc.: 59.47%] [G loss: 0.847809]\n",
            "764 [D loss: 0.623070, acc.: 60.23%] [G loss: 0.844547]\n",
            "765 [D loss: 0.643265, acc.: 62.12%] [G loss: 0.805700]\n",
            "766 [D loss: 0.647264, acc.: 60.61%] [G loss: 0.805999]\n",
            "767 [D loss: 0.617780, acc.: 67.80%] [G loss: 0.837777]\n",
            "768 [D loss: 0.613203, acc.: 71.97%] [G loss: 0.867178]\n",
            "769 [D loss: 0.637266, acc.: 66.29%] [G loss: 0.857838]\n",
            "770 [D loss: 0.613773, acc.: 72.35%] [G loss: 0.831239]\n",
            "771 [D loss: 0.638805, acc.: 62.88%] [G loss: 0.813322]\n",
            "772 [D loss: 0.646545, acc.: 59.85%] [G loss: 0.826794]\n",
            "773 [D loss: 0.620700, acc.: 64.77%] [G loss: 0.836283]\n",
            "774 [D loss: 0.616759, acc.: 72.35%] [G loss: 0.827593]\n",
            "775 [D loss: 0.606321, acc.: 68.56%] [G loss: 0.856224]\n",
            "776 [D loss: 0.606386, acc.: 72.73%] [G loss: 0.871541]\n",
            "777 [D loss: 0.608429, acc.: 72.73%] [G loss: 0.863939]\n",
            "778 [D loss: 0.606308, acc.: 70.08%] [G loss: 0.830723]\n",
            "779 [D loss: 0.589017, acc.: 71.97%] [G loss: 0.808435]\n",
            "780 [D loss: 0.603977, acc.: 68.56%] [G loss: 0.802374]\n",
            "781 [D loss: 0.589636, acc.: 67.42%] [G loss: 0.834463]\n",
            "782 [D loss: 0.591199, acc.: 70.45%] [G loss: 0.827324]\n",
            "783 [D loss: 0.607581, acc.: 69.70%] [G loss: 0.849202]\n",
            "784 [D loss: 0.604941, acc.: 70.08%] [G loss: 0.843792]\n",
            "785 [D loss: 0.628105, acc.: 69.32%] [G loss: 0.834943]\n",
            "786 [D loss: 0.606909, acc.: 70.83%] [G loss: 0.836831]\n",
            "787 [D loss: 0.619506, acc.: 66.29%] [G loss: 0.830606]\n",
            "788 [D loss: 0.633393, acc.: 65.15%] [G loss: 0.840922]\n",
            "789 [D loss: 0.608516, acc.: 70.83%] [G loss: 0.857401]\n",
            "790 [D loss: 0.611205, acc.: 71.21%] [G loss: 0.837464]\n",
            "791 [D loss: 0.596383, acc.: 73.48%] [G loss: 0.847181]\n",
            "792 [D loss: 0.600900, acc.: 73.11%] [G loss: 0.841618]\n",
            "793 [D loss: 0.618242, acc.: 68.18%] [G loss: 0.856006]\n",
            "794 [D loss: 0.623365, acc.: 69.32%] [G loss: 0.870067]\n",
            "795 [D loss: 0.621762, acc.: 65.53%] [G loss: 0.859078]\n",
            "796 [D loss: 0.622997, acc.: 67.80%] [G loss: 0.855926]\n",
            "797 [D loss: 0.632692, acc.: 65.15%] [G loss: 0.845960]\n",
            "798 [D loss: 0.618877, acc.: 67.05%] [G loss: 0.878391]\n",
            "799 [D loss: 0.606597, acc.: 67.80%] [G loss: 0.881347]\n",
            "800 [D loss: 0.612723, acc.: 68.56%] [G loss: 0.880917]\n",
            "801 [D loss: 0.630946, acc.: 66.67%] [G loss: 0.857419]\n",
            "802 [D loss: 0.609065, acc.: 64.02%] [G loss: 0.869544]\n",
            "803 [D loss: 0.631430, acc.: 65.15%] [G loss: 0.864994]\n",
            "804 [D loss: 0.632821, acc.: 60.61%] [G loss: 0.857698]\n",
            "805 [D loss: 0.622544, acc.: 62.12%] [G loss: 0.862949]\n",
            "806 [D loss: 0.616302, acc.: 71.59%] [G loss: 0.882849]\n",
            "807 [D loss: 0.606079, acc.: 72.73%] [G loss: 0.880047]\n",
            "808 [D loss: 0.600839, acc.: 71.59%] [G loss: 0.884747]\n",
            "809 [D loss: 0.599159, acc.: 68.56%] [G loss: 0.901565]\n",
            "810 [D loss: 0.605189, acc.: 68.94%] [G loss: 0.918531]\n",
            "811 [D loss: 0.626423, acc.: 69.70%] [G loss: 0.882963]\n",
            "812 [D loss: 0.603842, acc.: 72.35%] [G loss: 0.862266]\n",
            "813 [D loss: 0.599299, acc.: 73.11%] [G loss: 0.868652]\n",
            "814 [D loss: 0.613192, acc.: 68.94%] [G loss: 0.877055]\n",
            "815 [D loss: 0.617730, acc.: 68.56%] [G loss: 0.853670]\n",
            "816 [D loss: 0.601516, acc.: 73.86%] [G loss: 0.870376]\n",
            "817 [D loss: 0.609562, acc.: 67.05%] [G loss: 0.853177]\n",
            "818 [D loss: 0.616144, acc.: 68.94%] [G loss: 0.893257]\n",
            "819 [D loss: 0.601951, acc.: 74.24%] [G loss: 0.905227]\n",
            "820 [D loss: 0.584314, acc.: 81.44%] [G loss: 0.899850]\n",
            "821 [D loss: 0.602397, acc.: 71.59%] [G loss: 0.872149]\n",
            "822 [D loss: 0.619418, acc.: 64.02%] [G loss: 0.862971]\n",
            "823 [D loss: 0.596408, acc.: 67.42%] [G loss: 0.874282]\n",
            "824 [D loss: 0.597836, acc.: 72.73%] [G loss: 0.866258]\n",
            "825 [D loss: 0.616111, acc.: 66.67%] [G loss: 0.873163]\n",
            "826 [D loss: 0.596855, acc.: 74.62%] [G loss: 0.846269]\n",
            "827 [D loss: 0.603130, acc.: 66.67%] [G loss: 0.842988]\n",
            "828 [D loss: 0.606737, acc.: 68.18%] [G loss: 0.855108]\n",
            "829 [D loss: 0.599182, acc.: 71.21%] [G loss: 0.855200]\n",
            "830 [D loss: 0.605633, acc.: 68.94%] [G loss: 0.856730]\n",
            "831 [D loss: 0.585822, acc.: 72.35%] [G loss: 0.873404]\n",
            "832 [D loss: 0.589046, acc.: 77.27%] [G loss: 0.879252]\n",
            "833 [D loss: 0.609168, acc.: 71.21%] [G loss: 0.864934]\n",
            "834 [D loss: 0.590776, acc.: 71.59%] [G loss: 0.854479]\n",
            "835 [D loss: 0.612398, acc.: 66.67%] [G loss: 0.831944]\n",
            "836 [D loss: 0.591681, acc.: 68.18%] [G loss: 0.846702]\n",
            "837 [D loss: 0.596307, acc.: 66.67%] [G loss: 0.816540]\n",
            "838 [D loss: 0.599727, acc.: 64.02%] [G loss: 0.821419]\n",
            "839 [D loss: 0.588726, acc.: 70.45%] [G loss: 0.818248]\n",
            "840 [D loss: 0.606646, acc.: 65.91%] [G loss: 0.847884]\n",
            "841 [D loss: 0.596925, acc.: 67.42%] [G loss: 0.870540]\n",
            "842 [D loss: 0.605850, acc.: 70.08%] [G loss: 0.851732]\n",
            "843 [D loss: 0.616914, acc.: 68.18%] [G loss: 0.826476]\n",
            "844 [D loss: 0.613050, acc.: 62.50%] [G loss: 0.834093]\n",
            "845 [D loss: 0.617664, acc.: 63.64%] [G loss: 0.861042]\n",
            "846 [D loss: 0.609557, acc.: 67.05%] [G loss: 0.871558]\n",
            "847 [D loss: 0.622056, acc.: 63.26%] [G loss: 0.869879]\n",
            "848 [D loss: 0.617297, acc.: 65.53%] [G loss: 0.874872]\n",
            "849 [D loss: 0.595098, acc.: 68.94%] [G loss: 0.908373]\n",
            "850 [D loss: 0.584612, acc.: 71.59%] [G loss: 0.889583]\n",
            "851 [D loss: 0.621349, acc.: 65.91%] [G loss: 0.902616]\n",
            "852 [D loss: 0.583928, acc.: 73.11%] [G loss: 0.910503]\n",
            "853 [D loss: 0.612047, acc.: 68.94%] [G loss: 0.901529]\n",
            "854 [D loss: 0.597896, acc.: 71.21%] [G loss: 0.903518]\n",
            "855 [D loss: 0.605703, acc.: 68.56%] [G loss: 0.874906]\n",
            "856 [D loss: 0.588493, acc.: 70.45%] [G loss: 0.881698]\n",
            "857 [D loss: 0.619391, acc.: 68.18%] [G loss: 0.866681]\n",
            "858 [D loss: 0.620052, acc.: 64.77%] [G loss: 0.887751]\n",
            "859 [D loss: 0.594994, acc.: 72.73%] [G loss: 0.901697]\n",
            "860 [D loss: 0.606778, acc.: 67.05%] [G loss: 0.911622]\n",
            "861 [D loss: 0.619822, acc.: 66.67%] [G loss: 0.900401]\n",
            "862 [D loss: 0.638926, acc.: 59.47%] [G loss: 0.885775]\n",
            "863 [D loss: 0.623166, acc.: 65.53%] [G loss: 0.901745]\n",
            "864 [D loss: 0.602521, acc.: 70.83%] [G loss: 0.880075]\n",
            "865 [D loss: 0.608186, acc.: 65.15%] [G loss: 0.904616]\n",
            "866 [D loss: 0.623451, acc.: 67.42%] [G loss: 0.879428]\n",
            "867 [D loss: 0.628046, acc.: 65.15%] [G loss: 0.881169]\n",
            "868 [D loss: 0.612060, acc.: 69.32%] [G loss: 0.891931]\n",
            "869 [D loss: 0.629619, acc.: 68.18%] [G loss: 0.922007]\n",
            "870 [D loss: 0.620116, acc.: 64.39%] [G loss: 0.944010]\n",
            "871 [D loss: 0.615433, acc.: 69.32%] [G loss: 0.914593]\n",
            "872 [D loss: 0.659246, acc.: 55.30%] [G loss: 0.867247]\n",
            "873 [D loss: 0.631074, acc.: 58.71%] [G loss: 0.859936]\n",
            "874 [D loss: 0.625864, acc.: 62.88%] [G loss: 0.862372]\n",
            "875 [D loss: 0.613275, acc.: 70.08%] [G loss: 0.851443]\n",
            "876 [D loss: 0.634835, acc.: 60.61%] [G loss: 0.858869]\n",
            "877 [D loss: 0.642551, acc.: 59.09%] [G loss: 0.840252]\n",
            "878 [D loss: 0.639279, acc.: 62.12%] [G loss: 0.820301]\n",
            "879 [D loss: 0.623309, acc.: 65.53%] [G loss: 0.835828]\n",
            "880 [D loss: 0.618272, acc.: 63.64%] [G loss: 0.823303]\n",
            "881 [D loss: 0.632087, acc.: 62.88%] [G loss: 0.822929]\n",
            "882 [D loss: 0.625338, acc.: 64.02%] [G loss: 0.809137]\n",
            "883 [D loss: 0.633035, acc.: 62.50%] [G loss: 0.820430]\n",
            "884 [D loss: 0.615876, acc.: 71.59%] [G loss: 0.843792]\n",
            "885 [D loss: 0.618558, acc.: 65.91%] [G loss: 0.851497]\n",
            "886 [D loss: 0.613879, acc.: 70.45%] [G loss: 0.857768]\n",
            "887 [D loss: 0.612481, acc.: 68.56%] [G loss: 0.818906]\n",
            "888 [D loss: 0.614223, acc.: 68.18%] [G loss: 0.814438]\n",
            "889 [D loss: 0.613792, acc.: 67.05%] [G loss: 0.829135]\n",
            "890 [D loss: 0.602633, acc.: 68.18%] [G loss: 0.826621]\n",
            "891 [D loss: 0.620823, acc.: 64.77%] [G loss: 0.831968]\n",
            "892 [D loss: 0.618862, acc.: 67.05%] [G loss: 0.806504]\n",
            "893 [D loss: 0.635784, acc.: 64.02%] [G loss: 0.837295]\n",
            "894 [D loss: 0.615771, acc.: 66.67%] [G loss: 0.863222]\n",
            "895 [D loss: 0.623636, acc.: 68.56%] [G loss: 0.874335]\n",
            "896 [D loss: 0.617423, acc.: 70.83%] [G loss: 0.891509]\n",
            "897 [D loss: 0.613502, acc.: 67.05%] [G loss: 0.872643]\n",
            "898 [D loss: 0.642259, acc.: 64.39%] [G loss: 0.820340]\n",
            "899 [D loss: 0.630854, acc.: 57.95%] [G loss: 0.822338]\n",
            "900 [D loss: 0.620343, acc.: 62.50%] [G loss: 0.872860]\n",
            "901 [D loss: 0.603646, acc.: 70.45%] [G loss: 0.900487]\n",
            "902 [D loss: 0.616914, acc.: 68.18%] [G loss: 0.889256]\n",
            "903 [D loss: 0.606359, acc.: 75.76%] [G loss: 0.890653]\n",
            "904 [D loss: 0.610931, acc.: 71.59%] [G loss: 0.890762]\n",
            "905 [D loss: 0.604040, acc.: 70.83%] [G loss: 0.892072]\n",
            "906 [D loss: 0.594219, acc.: 71.97%] [G loss: 0.895970]\n",
            "907 [D loss: 0.597038, acc.: 72.73%] [G loss: 0.902926]\n",
            "908 [D loss: 0.581906, acc.: 76.89%] [G loss: 0.881305]\n",
            "909 [D loss: 0.606197, acc.: 68.18%] [G loss: 0.867190]\n",
            "910 [D loss: 0.575037, acc.: 75.76%] [G loss: 0.883224]\n",
            "911 [D loss: 0.604850, acc.: 70.83%] [G loss: 0.900211]\n",
            "912 [D loss: 0.607174, acc.: 69.32%] [G loss: 0.895053]\n",
            "913 [D loss: 0.593182, acc.: 68.94%] [G loss: 0.899189]\n",
            "914 [D loss: 0.621879, acc.: 66.67%] [G loss: 0.896222]\n",
            "915 [D loss: 0.608163, acc.: 68.56%] [G loss: 0.875056]\n",
            "916 [D loss: 0.600698, acc.: 67.42%] [G loss: 0.892411]\n",
            "917 [D loss: 0.586524, acc.: 71.97%] [G loss: 0.912954]\n",
            "918 [D loss: 0.637654, acc.: 60.98%] [G loss: 0.897585]\n",
            "919 [D loss: 0.627213, acc.: 65.53%] [G loss: 0.888438]\n",
            "920 [D loss: 0.607614, acc.: 72.35%] [G loss: 0.873040]\n",
            "921 [D loss: 0.619285, acc.: 67.42%] [G loss: 0.891826]\n",
            "922 [D loss: 0.623688, acc.: 67.80%] [G loss: 0.910847]\n",
            "923 [D loss: 0.611319, acc.: 69.32%] [G loss: 0.898979]\n",
            "924 [D loss: 0.611595, acc.: 70.08%] [G loss: 0.921193]\n",
            "925 [D loss: 0.606370, acc.: 71.59%] [G loss: 0.908576]\n",
            "926 [D loss: 0.597361, acc.: 71.21%] [G loss: 0.909666]\n",
            "927 [D loss: 0.621378, acc.: 67.05%] [G loss: 0.914023]\n",
            "928 [D loss: 0.598795, acc.: 68.56%] [G loss: 0.928710]\n",
            "929 [D loss: 0.596397, acc.: 71.97%] [G loss: 0.912010]\n",
            "930 [D loss: 0.623018, acc.: 61.74%] [G loss: 0.876934]\n",
            "931 [D loss: 0.640295, acc.: 62.12%] [G loss: 0.871147]\n",
            "932 [D loss: 0.615754, acc.: 65.91%] [G loss: 0.874322]\n",
            "933 [D loss: 0.607401, acc.: 68.94%] [G loss: 0.891487]\n",
            "934 [D loss: 0.608432, acc.: 72.35%] [G loss: 0.899608]\n",
            "935 [D loss: 0.621322, acc.: 68.18%] [G loss: 0.906452]\n",
            "936 [D loss: 0.640463, acc.: 66.29%] [G loss: 0.893058]\n",
            "937 [D loss: 0.611045, acc.: 69.70%] [G loss: 0.911042]\n",
            "938 [D loss: 0.634993, acc.: 64.77%] [G loss: 0.858660]\n",
            "939 [D loss: 0.616196, acc.: 68.56%] [G loss: 0.866227]\n",
            "940 [D loss: 0.610860, acc.: 67.80%] [G loss: 0.864536]\n",
            "941 [D loss: 0.602695, acc.: 73.11%] [G loss: 0.890717]\n",
            "942 [D loss: 0.603138, acc.: 68.18%] [G loss: 0.887508]\n",
            "943 [D loss: 0.613845, acc.: 66.67%] [G loss: 0.882159]\n",
            "944 [D loss: 0.621104, acc.: 67.05%] [G loss: 0.901396]\n",
            "945 [D loss: 0.611722, acc.: 70.08%] [G loss: 0.887098]\n",
            "946 [D loss: 0.614860, acc.: 67.80%] [G loss: 0.900655]\n",
            "947 [D loss: 0.617895, acc.: 69.70%] [G loss: 0.879803]\n",
            "948 [D loss: 0.595659, acc.: 73.86%] [G loss: 0.909124]\n",
            "949 [D loss: 0.595596, acc.: 77.27%] [G loss: 0.939490]\n",
            "950 [D loss: 0.617072, acc.: 71.21%] [G loss: 0.883389]\n",
            "951 [D loss: 0.599791, acc.: 75.00%] [G loss: 0.877751]\n",
            "952 [D loss: 0.607100, acc.: 76.89%] [G loss: 0.860998]\n",
            "953 [D loss: 0.605087, acc.: 71.59%] [G loss: 0.904054]\n",
            "954 [D loss: 0.594984, acc.: 72.73%] [G loss: 0.900320]\n",
            "955 [D loss: 0.602491, acc.: 72.73%] [G loss: 0.890608]\n",
            "956 [D loss: 0.611724, acc.: 69.32%] [G loss: 0.899163]\n",
            "957 [D loss: 0.586664, acc.: 78.79%] [G loss: 0.891860]\n",
            "958 [D loss: 0.610195, acc.: 72.35%] [G loss: 0.870024]\n",
            "959 [D loss: 0.595528, acc.: 72.35%] [G loss: 0.885374]\n",
            "960 [D loss: 0.586761, acc.: 76.52%] [G loss: 0.886152]\n",
            "961 [D loss: 0.600548, acc.: 73.11%] [G loss: 0.896482]\n",
            "962 [D loss: 0.604355, acc.: 75.00%] [G loss: 0.910608]\n",
            "963 [D loss: 0.605462, acc.: 74.24%] [G loss: 0.888668]\n",
            "964 [D loss: 0.592023, acc.: 73.11%] [G loss: 0.906169]\n",
            "965 [D loss: 0.596569, acc.: 75.00%] [G loss: 0.882773]\n",
            "966 [D loss: 0.606550, acc.: 72.35%] [G loss: 0.866486]\n",
            "967 [D loss: 0.590341, acc.: 73.48%] [G loss: 0.864450]\n",
            "968 [D loss: 0.578633, acc.: 76.89%] [G loss: 0.884454]\n",
            "969 [D loss: 0.573505, acc.: 73.48%] [G loss: 0.885949]\n",
            "970 [D loss: 0.601896, acc.: 70.45%] [G loss: 0.879858]\n",
            "971 [D loss: 0.603652, acc.: 67.05%] [G loss: 0.876177]\n",
            "972 [D loss: 0.616799, acc.: 65.91%] [G loss: 0.872177]\n",
            "973 [D loss: 0.605797, acc.: 70.83%] [G loss: 0.876291]\n",
            "974 [D loss: 0.584655, acc.: 75.38%] [G loss: 0.890165]\n",
            "975 [D loss: 0.610179, acc.: 69.70%] [G loss: 0.907981]\n",
            "976 [D loss: 0.618297, acc.: 70.83%] [G loss: 0.941381]\n",
            "977 [D loss: 0.580206, acc.: 81.44%] [G loss: 0.920812]\n",
            "978 [D loss: 0.597462, acc.: 74.62%] [G loss: 0.898677]\n",
            "979 [D loss: 0.578125, acc.: 76.89%] [G loss: 0.909876]\n",
            "980 [D loss: 0.577213, acc.: 78.03%] [G loss: 0.923583]\n",
            "981 [D loss: 0.585827, acc.: 78.79%] [G loss: 0.909159]\n",
            "982 [D loss: 0.572763, acc.: 84.85%] [G loss: 0.927815]\n",
            "983 [D loss: 0.582481, acc.: 76.52%] [G loss: 0.943698]\n",
            "984 [D loss: 0.593098, acc.: 71.59%] [G loss: 0.906082]\n",
            "985 [D loss: 0.598803, acc.: 73.86%] [G loss: 0.894614]\n",
            "986 [D loss: 0.568332, acc.: 78.41%] [G loss: 0.915861]\n",
            "987 [D loss: 0.576203, acc.: 76.52%] [G loss: 0.922446]\n",
            "988 [D loss: 0.568540, acc.: 80.30%] [G loss: 0.914607]\n",
            "989 [D loss: 0.597919, acc.: 75.76%] [G loss: 0.911323]\n",
            "990 [D loss: 0.615874, acc.: 66.29%] [G loss: 0.874929]\n",
            "991 [D loss: 0.581126, acc.: 75.00%] [G loss: 0.914697]\n",
            "992 [D loss: 0.574343, acc.: 75.00%] [G loss: 0.915813]\n",
            "993 [D loss: 0.613652, acc.: 66.29%] [G loss: 0.915726]\n",
            "994 [D loss: 0.599131, acc.: 69.32%] [G loss: 0.904761]\n",
            "995 [D loss: 0.620469, acc.: 65.15%] [G loss: 0.891749]\n",
            "996 [D loss: 0.629192, acc.: 63.64%] [G loss: 0.886920]\n",
            "997 [D loss: 0.627889, acc.: 65.53%] [G loss: 0.864791]\n",
            "998 [D loss: 0.595503, acc.: 70.08%] [G loss: 0.874525]\n",
            "999 [D loss: 0.622443, acc.: 71.59%] [G loss: 0.909455]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}