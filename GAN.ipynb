{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z4HRA-S/GAN-vs-MiniBatch-GAN/blob/master/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMRRYh5E-ySj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "1d6e6fac-0fcb-4e15-eaa1-fc15d3694086"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 21kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (49.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=178ce2e5f9ecb7c8af82625aac8b1ead760a2c8670213fa93eca37377537a182\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DQN5j6okdPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/ \n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgooArFzAiZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc2ef658-c163-4976-e7b1-0379a76184dc"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras import initializers\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "class MinibatchDiscrimination(Layer):\n",
        "    def __init__(self, nb_kernels, kernel_dim, init='random_normal', weights=None,input_dim=None, **kwargs):\n",
        "        self.init = initializers.get(init)\n",
        "        self.nb_kernels = nb_kernels\n",
        "        self.kernel_dim = kernel_dim\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = [InputSpec(ndim=2)]\n",
        "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = [InputSpec(dtype=K.floatx(),shape=(None, input_dim))]\n",
        "        self.W = self.add_weight(shape=(self.nb_kernels, input_dim, self.kernel_dim),\n",
        "        initializer=self.init,\n",
        "        name='kernel',\n",
        "        trainable=True)\n",
        "        # Set built to true.\n",
        "        super(MinibatchDiscrimination, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        activation = K.reshape(K.dot(x, self.W), (-1, self.nb_kernels, self.kernel_dim))\n",
        "        diffs = K.expand_dims(activation, 3) - K.expand_dims(K.permute_dimensions(activation, [1, 2, 0]), 0)\n",
        "        abs_diffs = K.sum(K.abs(diffs), axis=2)\n",
        "        minibatch_features = K.sum(K.exp(-abs_diffs), axis=2)\n",
        "        return K.concatenate([x, minibatch_features], 1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], input_shape[1]+self.nb_kernels\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'nb_kernels': self.nb_kernels,\n",
        "                      'kernel_dim': self.kernel_dim,\n",
        "                      'init': self.init.__name__,\n",
        "                      'input_dim': self.input_dim}\n",
        "        base_config = super(MinibatchDiscrimination, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self, minibatch = False):\n",
        "        self.minibatch = minibatch\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        validity = self.discriminator(img)\n",
        "\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "        model.summary()\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "        return Model(noise, img)\n",
        "        \n",
        "    def build_discriminator(self):\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=self.img_shape))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        if self.minibatch:\n",
        "            model.add(MinibatchDiscrimination(5,3))\n",
        "\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "        return Model(img, validity)\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        (X_train, _), (_, _) = mnist.load_data()\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "            if epoch % sample_interval == 0:\n",
        "               self.sample_images(epoch)\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        #fig.savefig(\"images/%d.png\" % epoch)\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gan = GAN(minibatch=True)\n",
        "    #tensorboard_callback.set_model(gan)\n",
        "    gan.train(epochs=1000, batch_size=132, sample_interval=10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "minibatch_discrimination_2 ( (None, 261)               3840      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 262       \n",
            "=================================================================\n",
            "Total params: 537,350\n",
            "Trainable params: 537,350\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_11 (Dense)             (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 784)               803600    \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-214edf0a8193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m132\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mset_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_run_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malways_record_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m               \u001b[0msummary_ops_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GAN' object has no attribute 'run_eagerly'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_YRSTHGlGil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}